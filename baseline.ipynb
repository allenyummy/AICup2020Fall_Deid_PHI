{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "* Change input data (ex. train.txt) into CRF model input format (ex. train.data)\n",
    "    * CRF model input format (ex. train.data):\n",
    "        ```\n",
    "        肝 O\n",
    "        功 O\n",
    "        能 O\n",
    "        6 B-med_exam\n",
    "        8 I-med_exam\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path='data/orig/train_1_update.txt'\n",
    "dev_file_path='data/orig/SampleData_deid.txt'\n",
    "test_file_path='data/orig/development_1.txt'\n",
    "\n",
    "out_train_file_path='data/baseline/train_1_update.txt'\n",
    "out_dev_file_path='data/baseline/SampleData_deid.txt'\n",
    "out_test_file_path='data/sl/development_1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadInputFile(path):\n",
    "    trainingset = list()  # store trainingset [content,content,...]\n",
    "    position = list()  # store position [article_id, start_pos, end_pos, entity_text, entity_type, ...]\n",
    "    mentions = dict()  # store mentions[mention] = Type\n",
    "    with open(file_path, 'r', encoding='utf8') as f:\n",
    "        file_text=f.read().encode('utf-8').decode('utf-8-sig')\n",
    "    datas=file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
    "    for data in datas:\n",
    "        data=data.split('\\n')\n",
    "        content=data[0]\n",
    "        trainingset.append(content)\n",
    "        annotations=data[1:]\n",
    "        for annot in annotations[1:]:\n",
    "            annot=annot.split('\\t') #annot= article_id, start_pos, end_pos, entity_text, entity_type\n",
    "            position.extend(annot)\n",
    "            mentions[annot[3]]=annot[4]\n",
    "    \n",
    "    return trainingset, position, mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRFFormatData(trainingset, position, path):\n",
    "    if (os.path.isfile(path)):\n",
    "        os.remove(path)\n",
    "    outputfile = open(path, 'a', encoding= 'utf-8')\n",
    "\n",
    "    # output file lines\n",
    "    count = 0 # annotation counts in each content\n",
    "    tagged = list()\n",
    "    for article_id in range(len(trainingset)):\n",
    "        trainingset_split = list(trainingset[article_id])\n",
    "        while '' or ' ' in trainingset_split:\n",
    "            if '' in trainingset_split:\n",
    "                trainingset_split.remove('')\n",
    "            else:\n",
    "                trainingset_split.remove(' ')\n",
    "        start_tmp = 0\n",
    "        for position_idx in range(0,len(position),5):\n",
    "            if int(position[position_idx]) == article_id:\n",
    "                count += 1\n",
    "                if count == 1:\n",
    "                    start_pos = int(position[position_idx+1])\n",
    "                    end_pos = int(position[position_idx+2])\n",
    "                    entity_type=position[position_idx+4]\n",
    "                    if start_pos == 0:\n",
    "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            # BIO states\n",
    "                            if token_idx == 0:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                            \n",
    "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    else:\n",
    "                        token = list(trainingset[article_id][0:start_pos])\n",
    "                        whole_token = trainingset[article_id][0:start_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            # BIO states\n",
    "                            if token[0] == '':\n",
    "                                if token_idx == 1:\n",
    "                                    label = 'B-'+entity_type\n",
    "                                else:\n",
    "                                    label = 'I-'+entity_type\n",
    "                            else:\n",
    "                                if token_idx == 0:\n",
    "                                    label = 'B-'+entity_type\n",
    "                                else:\n",
    "                                    label = 'I-'+entity_type\n",
    "\n",
    "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    start_tmp = end_pos\n",
    "                else:\n",
    "                    start_pos = int(position[position_idx+1])\n",
    "                    end_pos = int(position[position_idx+2])\n",
    "                    entity_type=position[position_idx+4]\n",
    "                    if start_pos<start_tmp:\n",
    "                        continue\n",
    "                    else:\n",
    "                        token = list(trainingset[article_id][start_tmp:start_pos])\n",
    "                        whole_token = trainingset[article_id][start_tmp:start_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                    whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                    for token_idx in range(len(token)):\n",
    "                        if len(token[token_idx].replace(' ','')) == 0:\n",
    "                            continue\n",
    "                        # BIO states\n",
    "                        if token[0] == '':\n",
    "                            if token_idx == 1:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                        else:\n",
    "                            if token_idx == 0:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                        \n",
    "                        output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                        outputfile.write(output_str)\n",
    "                    start_tmp = end_pos\n",
    "\n",
    "        token = list(trainingset[article_id][start_tmp:])\n",
    "        whole_token = trainingset[article_id][start_tmp:]\n",
    "        for token_idx in range(len(token)):\n",
    "            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "            outputfile.write(output_str)\n",
    "\n",
    "        count = 0\n",
    "    \n",
    "        output_str = '\\n'\n",
    "        outputfile.write(output_str)\n",
    "        ID = trainingset[article_id]\n",
    "\n",
    "        if article_id%10 == 0:\n",
    "            print('Total complete articles:', article_id)\n",
    "\n",
    "    # close output file\n",
    "    outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total complete articles: 0\n",
      "Total complete articles: 10\n",
      "Total complete articles: 20\n",
      "Total complete articles: 30\n",
      "Total complete articles: 40\n",
      "Total complete articles: 50\n",
      "Total complete articles: 60\n",
      "Total complete articles: 70\n",
      "Total complete articles: 80\n",
      "Total complete articles: 90\n",
      "Total complete articles: 100\n",
      "Total complete articles: 110\n",
      "Total complete articles: 0\n",
      "Total complete articles: 10\n",
      "Total complete articles: 20\n"
     ]
    }
   ],
   "source": [
    "for file_path, out_file_path in zip([train_file_path, dev_file_path],[out_train_file_path, out_dev_file_path]):\n",
    "    trainingset, position, mentions=loadInputFile(file_path)\n",
    "    CRFFormatData(trainingset, position, out_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER model\n",
    "### CRF (Conditional Random Field model)\n",
    "* Using `sklearn-crfsuite` API\n",
    "\n",
    "    (you may try `CRF++`, `python-crfsuite`, `pytorch-crfsuite`(neural network version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite.metrics import flat_classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRF(x_train, y_train, x_test, y_test):\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "    crf.fit(x_train, y_train)\n",
    "    # print(crf)\n",
    "    y_pred = crf.predict(x_test)\n",
    "    y_pred_mar = crf.predict_marginals(x_test)\n",
    "\n",
    "    # print(y_pred_mar)\n",
    "\n",
    "    labels = list(crf.classes_)\n",
    "    labels.remove('O')\n",
    "    f1score = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)\n",
    "    sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n",
    "    print(flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))\n",
    "    return y_pred, y_pred_mar, f1score, crf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Input: \n",
    "* input features:\n",
    "    * word vector: pretrained traditional chinese word embedding by Word2Vec-CBOW\n",
    "    \n",
    "    (you may try add some other features, ex. pos-tag, word_length, word_position, ...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained word vectors\n",
    "# get a dict of tokens (key) and their pretrained word vectors (value)\n",
    "# pretrained word2vec CBOW word vector: https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1\n",
    "dim = 0\n",
    "word_vecs= {}\n",
    "# open pretrained word vector file\n",
    "with open('data/baseline/cbow_word_vector/cna.cbow.cwe_p.tar_g.512d.0.txt') as f:\n",
    "    for line in f:\n",
    "        tokens = line.strip().split()\n",
    "\n",
    "        # there 2 integers in the first line: vocabulary_size, word_vector_dim\n",
    "        if len(tokens) == 2:\n",
    "            dim = int(tokens[1])\n",
    "            continue\n",
    "    \n",
    "        word = tokens[0] \n",
    "        vec = np.array([ float(t) for t in tokens[1:] ])\n",
    "        word_vecs[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vocabulary_size:  158566  word_vector_dim:  (512,)\n"
     ]
    }
   ],
   "source": [
    "print('vocabulary_size: ',len(word_vecs),' word_vector_dim: ',vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split data into training dataset and testing dataset,\n",
    "however, we'll provide `development data` and `test data` which is real testing dataset.\n",
    "\n",
    "You should upload prediction on `development data` and `test data` to system, not this splitted testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load `train.data` and separate into a list of labeled data of each text\n",
    "# return:\n",
    "#   data_list: a list of lists of tuples, storing tokens and labels (wrapped in tuple) of each text in `train.data`\n",
    "#   traindata_list: a list of lists, storing training data_list splitted from data_list\n",
    "#   testdata_list: a list of lists, storing testing data_list splitted from data_list\n",
    "from sklearn.model_selection import train_test_split\n",
    "def Dataset(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data=f.readlines()#.encode('utf-8').decode('utf-8-sig')\n",
    "    data_list, data_list_tmp = list(), list()\n",
    "    article_id_list=list()\n",
    "    idx=0\n",
    "    for row in data:\n",
    "        data_tuple = tuple()\n",
    "        if row == '\\n' or row == ' \\n':\n",
    "            article_id_list.append(idx)\n",
    "            idx+=1\n",
    "            data_list.append(data_list_tmp)\n",
    "            data_list_tmp = []\n",
    "        else:\n",
    "            row = row.strip('\\n').split(' ')\n",
    "            if len(row) == 2:\n",
    "                data_tuple = (row[0], row[1])\n",
    "            elif len(row) == 1:\n",
    "                data_tuple = (row[0], )\n",
    "            data_list_tmp.append(data_tuple)\n",
    "    if len(data_list_tmp) != 0:\n",
    "        data_list.append(data_list_tmp)\n",
    "    \n",
    "    # # here we random split data into training dataset and testing dataset\n",
    "    # # but you should take `development data` or `test data` as testing data\n",
    "    # # At that time, you could just delete this line, \n",
    "    # # and generate data_list of `train data` and data_list of `development/test data` by this function\n",
    "    # traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list=train_test_split(data_list,\n",
    "    #                                                                                                 article_id_list,\n",
    "    #                                                                                                 test_size=0.2,\n",
    "    #                                                                                                 random_state=42)\n",
    "    \n",
    "    return data_list, article_id_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up word vectors\n",
    "# turn each word into its pretrained word vector\n",
    "# return a list of word vectors corresponding to each token in train.data\n",
    "def Word2Vector(data_list, embedding_dict):\n",
    "    embedding_list = list()\n",
    "\n",
    "    # No Match Word (unknown word) Vector in Embedding\n",
    "    unk_vector=np.random.rand(*(list(embedding_dict.values())[0].shape))\n",
    "\n",
    "    for idx_list in range(len(data_list)):\n",
    "        embedding_list_tmp = list()\n",
    "        for idx_tuple in range(len(data_list[idx_list])):\n",
    "            key = data_list[idx_list][idx_tuple][0] # token\n",
    "\n",
    "            if key in embedding_dict:\n",
    "                value = embedding_dict[key]\n",
    "            else:\n",
    "                value = unk_vector\n",
    "            embedding_list_tmp.append(value)\n",
    "        embedding_list.append(embedding_list_tmp)\n",
    "    return embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input features: pretrained word vectors of each token\n",
    "# return a list of feature dicts, each feature dict corresponding to each token\n",
    "def Feature(embed_list):\n",
    "    feature_list = list()\n",
    "    for idx_list in range(len(embed_list)):\n",
    "        feature_list_tmp = list()\n",
    "        for idx_tuple in range(len(embed_list[idx_list])):\n",
    "            feature_dict = dict()\n",
    "            for idx_vec in range(len(embed_list[idx_list][idx_tuple])):\n",
    "                feature_dict['dim_' + str(idx_vec+1)] = embed_list[idx_list][idx_tuple][idx_vec]\n",
    "            feature_list_tmp.append(feature_dict)\n",
    "        feature_list.append(feature_list_tmp)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels of each tokens in train.data\n",
    "# return a list of lists of labels\n",
    "def Preprocess(data_list):\n",
    "    label_list = list()\n",
    "    for idx_list in range(len(data_list)):\n",
    "        label_list_tmp = list()\n",
    "        for idx_tuple in range(len(data_list[idx_list])):\n",
    "            label_list_tmp.append(data_list[idx_list][idx_tuple][1])\n",
    "        label_list.append(label_list_tmp)\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_list, traindata_article_id_list = Dataset(out_train_file_path)\n",
    "devdata_list, devdata_article_id_list = Dataset(out_dev_file_path)\n",
    "testdata_list, testdata_article_id_list = Dataset(out_test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word Embedding\n",
    "trainembed_list = Word2Vector(traindata_list, word_vecs)\n",
    "devembed_list = Word2Vector(devdata_list, word_vecs)\n",
    "testembed_list = Word2Vector(testdata_list, word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRF - Train Data (Augmentation Data)\n",
    "x_train = Feature(trainembed_list)\n",
    "y_train = Preprocess(traindata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRF - Dev Data (Golden Standard)\n",
    "x_dev = Feature(devembed_list)\n",
    "y_dev = Preprocess(devdata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRF - Test Data\n",
    "x_test = Feature(testembed_list)\n",
    "# y_test = Preprocess(testdata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_pred_mar, f1score, crf = CRF(x_train, y_train, x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('outputs/crf.pickle', 'wb') as f:\n",
    "    pickle.dump(crf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('outputs/crf.pickle', 'rb') as f:\n",
    "    crf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = crf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3373\n800\n1482\n3291\n2132\n2856\n1646\n1116\n960\n760\n556\n2364\n955\n1183\n1404\n1926\n723\n3154\n655\n6065\n4665\n3494\n3933\n1873\n1047\n1270\n2592\n637\n2654\n2582\n1682\n2147\n2848\n978\n4616\n5113\n907\n1460\n715\n2234\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_pred[20])):\n",
    "    print (y_pred[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sl import write_predictions_to_file\n",
    "with open(\"outputs/test_predictions_baseline_crf.txt\", \"w\", encoding=\"utf-8\") as writer:\n",
    "    with open(out_test_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        write_predictions_to_file(writer, f, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output data\n",
    "* Change model output into `output.tsv` \n",
    "* Only accept this output format uploading to competition system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output=\"article_id\\tstart_position\\tend_position\\tentity_text\\tentity_type\\n\"\n",
    "for test_id in range(len(y_pred)):\n",
    "    pos=0\n",
    "    start_pos=None\n",
    "    end_pos=None\n",
    "    entity_text=None\n",
    "    entity_type=None\n",
    "    for pred_id in range(len(y_pred[test_id])):\n",
    "        if y_pred[test_id][pred_id][0]=='B':\n",
    "            start_pos=pos\n",
    "            entity_type=y_pred[test_id][pred_id][2:]\n",
    "        elif start_pos is not None and y_pred[test_id][pred_id][0]=='I' and pred_id+1 < len(y_pred[test_id]) and y_pred[test_id][pred_id+1][0]=='O':\n",
    "            end_pos=pos\n",
    "            entity_text=''.join([testdata_list[test_id][position][0] for position in range(start_pos,end_pos+1)])\n",
    "            line=str(testdata_article_id_list[test_id])+'\\t'+str(start_pos)+'\\t'+str(end_pos+1)+'\\t'+entity_text+'\\t'+entity_type\n",
    "            output+=line+'\\n'\n",
    "        pos+=1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='outputs/test_predictions_baseline_crf.tsv'\n",
    "with open(output_path,'w',encoding='utf-8') as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "article_id\tstart_position\tend_position\tentity_text\tentity_type\n0\t2624\t2628\t下下禮拜\ttime\n0\t2734\t2740\t在布魯克林區\tlocation\n0\t2780\t2782\t北區\tlocation\n0\t2807\t2810\t下禮拜\ttime\n0\t3029\t3031\t昨天\ttime\n0\t3067\t3070\t下禮拜\ttime\n0\t3340\t3343\t下禮拜\ttime\n1\t457\t460\t五月中\ttime\n1\t671\t674\t三個月\ttime\n2\t40\t45\t上上個禮拜\ttime\n2\t1139\t1141\t爸爸\tfamily\n2\t1147\t1150\t三個月\ttime\n2\t1227\t1231\t兩個禮拜\ttime\n3\t576\t580\t6月4號\ttime\n3\t582\t584\t前年\ttime\n3\t592\t594\t前年\ttime\n3\t1929\t1931\t四月\ttime\n3\t1932\t1934\t五月\ttime\n3\t2178\t2180\t八月\ttime\n3\t2195\t2197\t八月\ttime\n3\t2204\t2207\t八月份\ttime\n3\t2230\t2233\t八月份\ttime\n4\t146\t148\t昨天\ttime\n4\t187\t190\t林醫師\tname\n4\t318\t321\t巫醫師\tname\n4\t338\t343\t前一個禮拜\ttime\n4\t354\t358\t上個禮拜\ttime\n4\t623\t625\t昨天\ttime\n4\t666\t668\t昨天\ttime\n4\t731\t734\t林醫師\tname\n4\t806\t809\t林醫師\tname\n4\t815\t818\t林醫師\tname\n4\t838\t840\t昨天\ttime\n4\t844\t847\t林醫師\tname\n4\t880\t883\t林醫師\tname\n4\t1051\t1054\t呂醫師\tname\n4\t1537\t1540\t許醫師\tname\n4\t1606\t1609\t林醫師\tname\n4\t1620\t1623\t林醫師\tname\n4\t1665\t1668\t林醫師\tname\n4\t1704\t1706\t昨天\ttime\n4\t1720\t1723\t巫醫師\tname\n4\t1724\t1729\t巫大維醫師\tname\n4\t1811\t1814\t林醫師\tname\n4\t1912\t1915\t巫醫師\tname\n4\t1960\t1963\t三個月\ttime\n4\t1967\t1970\t三個月\ttime\n4\t1974\t1977\t三個月\ttime\n4\t1979\t1982\t三個月\ttime\n4\t1988\t1991\t三個月\ttime\n5\t83\t87\t今天早上\ttime\n5\t1047\t1051\t昨天晚上\ttime\n5\t1886\t1889\t禮拜三\ttime\n5\t1893\t1896\t禮拜三\ttime\n5\t1919\t1925\t上午，禮拜三\ttime\n5\t2433\t2436\t禮拜三\ttime\n5\t2437\t2441\t6月6號\ttime\n6\t28\t32\t上個禮拜\ttime\n6\t86\t88\t前年\ttime\n6\t158\t162\t今年5月\ttime\n6\t376\t380\t7月4號\ttime\n6\t802\t805\t三四天\ttime\n6\t822\t825\t三四天\ttime\n6\t829\t832\t三四天\ttime\n6\t895\t899\t兩個禮拜\ttime\n6\t989\t991\t昨天\ttime\n6\t1250\t1253\t吳醫師\tname\n6\t1625\t1627\t櫻木\tlocation\n7\t889\t892\t三個月\ttime\n7\t910\t913\t三個月\ttime\n7\t1038\t1041\t三個月\ttime\n7\t1045\t1048\t三個月\ttime\n7\t1055\t1061\t一個月一個月\ttime\n8\t215\t218\t黃醫師\tname\n8\t262\t265\t黃醫師\tname\n8\t533\t536\t黃醫師\tname\n8\t685\t689\t兩個禮拜\ttime\n8\t764\t767\t黃醫師\tname\n8\t932\t935\t王阿明\tname\n9\t35\t37\t五月\ttime\n10\t160\t164\t下下禮拜\ttime\n10\t175\t178\t下禮拜\ttime\n10\t181\t185\t下禮拜三\ttime\n10\t241\t245\t兩個禮拜\ttime\n10\t262\t266\t四個禮拜\ttime\n10\t373\t375\t四週\ttime\n10\t472\t476\t四個禮拜\ttime\n10\t491\t494\t禮拜三\ttime\n10\t537\t541\t十月六號\ttime\n11\t26\t29\t上禮拜\ttime\n11\t123\t125\t北榮\tlocation\n11\t132\t135\t林醫師\tname\n11\t143\t146\t林醫師\tname\n11\t154\t158\t紐約醫院\tlocation\n11\t174\t178\t紐約醫院\tlocation\n11\t210\t213\t林醫師\tname\n11\t271\t273\t阿美\tname\n11\t280\t282\t阿美\tname\n11\t313\t316\t林醫師\tname\n11\t362\t364\t北榮\tlocation\n11\t368\t371\t巫醫師\tname\n11\t376\t379\t王醫師\tname\n11\t417\t419\t北榮\tlocation\n11\t455\t458\t林醫師\tname\n11\t484\t486\t昨天\ttime\n11\t513\t516\t林醫師\tname\n11\t592\t594\t兩年\ttime\n11\t627\t630\t巫醫師\tname\n11\t676\t680\t紐約醫院\tlocation\n11\t709\t712\t巫醫師\tname\n11\t729\t732\t三個月\ttime\n11\t763\t766\t四個月\ttime\n11\t930\t933\t林醫師\tname\n11\t1094\t1097\t林醫師\tname\n11\t1153\t1156\t兩個月\ttime\n11\t1179\t1182\t兩個月\ttime\n11\t1233\t1236\t兩個月\ttime\n11\t1238\t1241\t兩個月\ttime\n11\t1273\t1276\t兩個月\ttime\n11\t1307\t1309\t彙整\tlocation\n11\t1353\t1355\t紐約\tlocation\n11\t1358\t1362\t紐約醫院\tlocation\n11\t1416\t1419\t兩個月\ttime\n11\t1470\t1472\t紐約\tlocation\n11\t1647\t1649\t兩年\ttime\n11\t1784\t1786\t北榮\tlocation\n11\t1806\t1808\t北榮\tlocation\n11\t1869\t1872\t林醫師\tname\n11\t2014\t2016\t今年\ttime\n11\t2029\t2032\t林醫師\tname\n11\t2347\t2350\t巫醫師\tname\n12\t15\t17\t延伸\tlocation\n13\t48\t52\t9月2號\ttime\n13\t493\t496\t0月底\ttime\n14\t1094\t1096\t今年\ttime\n14\t1136\t1140\t前兩個月\ttime\n16\t203\t207\t三個禮拜\ttime\n16\t218\t222\t六個禮拜\ttime\n16\t235\t239\t六個禮拜\ttime\n16\t257\t261\t六個禮拜\ttime\n16\t424\t427\t三個月\ttime\n16\t447\t449\t兩年\ttime\n16\t715\t719\t林大醫師\tname\n17\t1165\t1168\t巴的東\tlocation\n17\t1611\t1616\t今天禮拜天\ttime\n17\t1621\t1626\t今天禮拜天\ttime\n17\t1656\t1659\t禮拜二\ttime\n17\t1663\t1666\t禮拜二\ttime\n17\t1679\t1682\t禮拜二\ttime\n17\t2034\t2036\t五月\ttime\n17\t2038\t2041\t五月份\ttime\n17\t2062\t2065\t六月份\ttime\n17\t2167\t2169\t台中\tlocation\n17\t2185\t2187\t台中\tlocation\n17\t2344\t2346\t五月\ttime\n17\t2506\t2509\t前三天\ttime\n18\t21\t24\t五月中\ttime\n18\t429\t432\t三個月\ttime\n19\t821\t823\t叔叔\tfamily\n19\t832\t834\t叔叔\tfamily\n19\t2498\t2501\t下禮拜\ttime\n19\t3013\t3015\t陽明\tname\n19\t3082\t3084\t陽明\tname\n19\t3093\t3095\t陽明\tname\n19\t3837\t3839\t陽明\tname\n19\t3869\t3871\t陽明\tname\n19\t4040\t4043\t五顆星\ttime\n19\t4142\t4145\t六十歲\ttime\n19\t4543\t4546\t下禮拜\ttime\n19\t5007\t5009\t昨天\ttime\n19\t5298\t5300\t叔叔\tfamily\n19\t5487\t5490\t下禮拜\ttime\n19\t5625\t5628\t下禮拜\ttime\n19\t5949\t5952\t下禮拜\ttime\n19\t5967\t5969\t老爸\tfamily\n19\t6021\t6024\t下禮拜\ttime\n20\t1011\t1013\t兩年\ttime\n20\t1307\t1313\t三個月三個月\ttime\n20\t1314\t1317\t三個月\ttime\n20\t3013\t3016\t三個月\ttime\n20\t4646\t4650\t8月2號\ttime\n21\t209\t211\t一號\ttime\n21\t293\t295\t一號\ttime\n21\t590\t592\t歐美\tname\n21\t2553\t2555\t彰化\tlocation\n21\t2944\t2946\t兩週\ttime\n21\t2947\t2949\t六週\ttime\n22\t6\t9\t王醫師\tname\n22\t112\t115\t王醫師\tname\n22\t523\t525\t陽明\tname\n22\t538\t540\t陽明\tname\n22\t743\t745\t陽明\tname\n22\t981\t984\t四個月\ttime\n22\t989\t992\t四個月\ttime\n22\t2104\t2107\t三個月\ttime\n22\t2109\t2112\t六個月\ttime\n22\t2736\t2739\t王醫師\tname\n22\t3053\t3055\t日曆\tname\n22\t3351\t3354\t三個月\ttime\n22\t3484\t3488\t一萬多塊\tmoney\n22\t3576\t3579\t三個月\ttime\n22\t3604\t3607\t王醫師\tname\n22\t3619\t3622\t禮拜三\ttime\n22\t3641\t3644\t禮拜三\ttime\n22\t3702\t3705\t王醫師\tname\n23\t6\t9\t王醫師\tname\n23\t57\t60\t王醫師\tname\n23\t576\t579\t王醫師\tname\n23\t636\t639\t王醫師\tname\n23\t1756\t1759\t王醫師\tname\n24\t267\t270\t三四天\ttime\n24\t702\t705\t貞節牌\tname\n24\t716\t718\t台中\tlocation\n25\t159\t161\t二十\tmed_exam\n25\t1131\t1133\t五天\ttime\n25\t1222\t1225\t三四天\ttime\n26\t234\t236\t周末\ttime\n26\t243\t245\t平日\ttime\n26\t2339\t2342\t三個月\ttime\n26\t2542\t2545\t三個月\ttime\n26\t2569\t2572\t林小姐\tname\n27\t11\t13\t台中\tlocation\n27\t97\t99\t台中\tlocation\n27\t112\t114\t台中\tlocation\n27\t265\t267\t七天\ttime\n27\t355\t358\t上個月\ttime\n27\t398\t401\t十三天\ttime\n27\t451\t454\t十三天\ttime\n27\t517\t520\t十三天\ttime\n27\t589\t592\t這禮拜\ttime\n28\t1158\t1160\t昨天\ttime\n29\t51\t54\t小蘭姊\tlocation\n29\t256\t259\t三個月\ttime\n29\t380\t383\t三個月\ttime\n29\t559\t562\t三個月\ttime\n29\t1443\t1446\t三個月\ttime\n29\t1527\t1530\t三個月\ttime\n29\t1758\t1760\t北區\tlocation\n29\t2022\t2030\t第一個月7月6號\ttime\n29\t2124\t2128\t第一個月\ttime\n29\t2325\t2328\t三個月\ttime\n30\t250\t252\t嘉義\tlocation\n30\t444\t452\t今天禮拜、禮拜二\ttime\n30\t469\t472\t禮拜日\ttime\n30\t510\t513\t禮拜日\ttime\n30\t525\t528\t禮拜日\ttime\n30\t1260\t1262\t美國\tlocation\n31\t1924\t1928\t8月3號\ttime\n31\t2012\t2015\t三個月\ttime\n31\t2027\t2031\t第一個月\ttime\n32\t998\t1000\t直延\tlocation\n32\t1208\t1210\t小美\tname\n32\t1308\t1311\t三個月\ttime\n32\t1312\t1315\t六個月\ttime\n32\t1348\t1351\t三個月\ttime\n32\t1433\t1436\t三個月\ttime\n32\t1444\t1447\t五千塊\tmoney\n32\t1461\t1463\t五千\tmoney\n32\t1950\t1953\t五百多\tmed_exam\n32\t1967\t1969\t嘉義\tlocation\n32\t2198\t2201\t三個月\ttime\n32\t2211\t2214\t三個月\ttime\n32\t2294\t2296\t四週\ttime\n32\t2312\t2315\t三個月\ttime\n32\t2408\t2411\t三個月\ttime\n32\t2668\t2671\t三個月\ttime\n33\t44\t47\t禮拜二\ttime\n33\t51\t54\t禮拜五\ttime\n33\t874\t877\t下個月\ttime\n33\t881\t885\t0月7號\ttime\n33\t899\t902\t下禮拜\ttime\n33\t920\t923\t五下午\ttime\n34\t437\t439\t五年\ttime\n34\t772\t775\t五個月\ttime\n34\t779\t782\t五個月\ttime\n34\t1536\t1538\t昨天\ttime\n34\t2874\t2877\t三個月\ttime\n34\t4220\t4223\t下禮拜\ttime\n34\t4248\t4251\t禮拜二\ttime\n34\t4292\t4295\t禮拜三\ttime\n34\t4299\t4302\t禮拜五\ttime\n34\t4414\t4417\t禮拜三\ttime\n34\t4421\t4424\t禮拜五\ttime\n34\t4433\t4436\t禮拜五\ttime\n34\t4450\t4454\t下禮拜五\ttime\n34\t4459\t4462\t禮拜五\ttime\n34\t4482\t4486\t下禮拜五\ttime\n34\t4495\t4498\t禮拜五\ttime\n34\t4537\t4540\t下禮拜\ttime\n35\t168\t171\t陳小姐\tname\n35\t221\t224\t陳小姐\tname\n35\t1862\t1865\t第三天\ttime\n35\t2084\t2087\t間千萬\tmoney\n35\t2104\t2107\t正週末\ttime\n35\t2206\t2209\t一禮拜\ttime\n35\t3251\t3254\t九十五\tmed_exam\n35\t3485\t3489\t上三個月\ttime\n35\t3504\t3507\t上半年\ttime\n35\t4309\t4312\t一萬二\tmoney\n35\t4313\t4316\t一萬三\tmoney\n35\t4621\t4623\t美國\tlocation\n35\t4696\t4698\t七八\tmed_exam\n35\t5071\t5076\t下下個禮拜\ttime\n36\t17\t20\t六點半\ttime\n36\t28\t31\t六點半\ttime\n36\t43\t48\t七點四十二\tmed_exam\n36\t586\t590\t第七個月\ttime\n36\t601\t604\t下半年\ttime\n36\t632\t635\t下個月\ttime\n36\t660\t665\t四月二十五\ttime\n36\t678\t683\t四月二十五\ttime\n36\t715\t720\t四月二十五\ttime\n37\t848\t853\t第十一個月\ttime\n37\t1040\t1043\t一萬三\tmoney\n37\t1238\t1240\t美國\tlocation\n38\t70\t72\t高雄\tlocation\n38\t84\t86\t高雄\tlocation\n38\t92\t94\t高雄\tlocation\n38\t141\t143\t八號\ttime\n38\t160\t164\t兩個禮拜\ttime\n38\t177\t179\t高雄\tlocation\n38\t582\t584\t高雄\tlocation\n38\t641\t644\t下個月\ttime\n38\t648\t651\t下個月\ttime\n39\t141\t144\t三個月\ttime\n39\t770\t773\t六個月\ttime\n39\t1717\t1720\t三個月\ttime\n39\t2008\t2011\t六個月\ttime\n\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "* You may try `python-crfsuite` to train an neural network for NER tagging optimized by gradient descent back propagation\n",
    "    * [Documentation](https://github.com/scrapinghub/python-crfsuite)\n",
    "* You may try `CRF++` tool for NER tagging by CRF model\n",
    "    * [Documentation](http://taku910.github.io/crfpp/)\n",
    "    * Need design feature template\n",
    "    * Can only computed in CPU\n",
    "* You may try other traditional chinese word embedding (ex. fasttext, bert, ...) for input features\n",
    "* You may try add other features for NER model, ex. POS-tag, word_length, word_position, ...\n",
    "* You should upload the prediction output on `development data` or `test data` provided later to the competition system. Note don't upload prediction output on the splitted testing dataset like this baseline example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "baseline_crf = pd.read_csv(\"outputs/test_predictions_baseline_crf.tsv\", sep='\\t')\n",
    "bert = pd.read_csv(\"outputs/test_predictions.tsv\", sep='\\t')\n",
    "a = pd.concat([baseline_crf, bert])\n",
    "a.to_csv(\"outputs/merge.tsv\", index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('base': conda)",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}